{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing (NLP) Introduction:**\n",
        "Natural Language Processing (NLP) is an interdisciplinary field merging computer science, linguistics, and machine learning. Its main objective is to allow computers to comprehend and handle human language naturally and effectively. NLP tasks are generally divided into two key categories:\n",
        "\n",
        "**(i) Natural Language Understanding (NLU):** This focuses on interpreting and understanding human language, including tasks like speech recognition, text classification, sentiment analysis, and extracting information from text.\n",
        "\n",
        "**(ii) Natural Language Generation (NLG):** This focuses on producing human-readable text from structured data, involving tasks like text summarization, dialogue generation, and language translation.\n",
        "\n",
        "\n",
        "**Examples:** NLP is applied in many fields, such as customer support (through chatbots and virtual assistants), content analysis (like sentiment analysis and topic modeling), and information retrieval (such as search engines and question-answering systems), among others."
      ],
      "metadata": {
        "id": "j72afiIWb4k2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important NLP Libraries in Python:** Python provides a vast ecosystem of libraries and frameworks for performing NLP tasks. Below are some of the most popular and frequently used libraries:\n",
        "\n",
        "**(i) NLTK (Natural Language Toolkit):** NLTK is a highly versatile and widely used Python library for NLP tasks. It offers a comprehensive set of tools and resources for text processing, including functionalities like tokenization, stemming, lemmatization, part-of-speech tagging, and more, making it a go-to library for NLP practitioners.\n",
        "\n",
        "**(ii) spaCy:** spaCy is a high-performance library designed for advanced NLP tasks, offering powerful models for tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more. It is well-regarded for its speed and ability to be easily integrated into production environments.\n",
        "\n",
        "**(iii) TextBlob:** TextBlob is a user-friendly library built on top of NLTK and Pattern, designed to streamline common NLP tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more. It simplifies these processes, making it easier for users to implement NLP functionality in their projects.\n",
        "\n",
        "**(iv) Gensim:** Gensim is a powerful library designed for topic modeling, offering optimized algorithms like Latent Dirichlet Allocation (LDA), Word2Vec, and Doc2Vec. It excels in tasks such as topic discovery, text similarity analysis, and generating word embeddings for better text representation."
      ],
      "metadata": {
        "id": "jxCRn2yQ7bMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Data Preprocessing as well as Text Cleaning**\n",
        "\n",
        "Before applying NLP techniques, it's crucial to preprocess and clean the text data to achieve accurate and dependable outcomes. Below are some commonly used preprocessing steps."
      ],
      "metadata": {
        "id": "s8ZipFpJ-99G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a Python example of basic data preprocessing and text cleaning using the **Natural Language Toolkit (NLTK)** and **regular expressions (re)**."
      ],
      "metadata": {
        "id": "z2bcBduO-R0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps in the code:**\n",
        "\n",
        "**(i)Lowercasing:** Converts the text to lowercase.\n",
        "\n",
        "**(ii)Removing Punctuation:** Strips out any punctuation or special characters.\n",
        "\n",
        "**(iii)Tokenization:** Splits the text into individual words. For instance, the sentence “I love programming” would be tokenized into [\"I\", \"love\", \"programming\"].\n",
        "\n",
        "**(iv)Removing Stopwords:** Filters out common words like \"is\", \"and\", \"the\" that are not essential for NLP tasks.\n",
        "\n",
        "**(v)Lemmatization:** Reduces words to their base form (e.g., \"running\" → \"run\")."
      ],
      "metadata": {
        "id": "AXXns6CR9wIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re                           #Imports the regular expression module, which provides tools for text pattern matching and manipulation (used here to remove punctuation).\n",
        "import nltk                         #Imports the Natural Language Toolkit (NLTK) to provide tools for tokenization, lemmatization, and more.\n",
        "from nltk.corpus import stopwords   #Imports the stopwords corpus from NLTK, which contains a list of common words (such as \"the\", \"and\", \"in\") that are often removed in text preprocessing because they do not carry significant meaning.\n",
        "from nltk.tokenize import word_tokenize   #Imports the word_tokenize function from NLTK, which splits a text into individual words (tokens).\n",
        "from nltk.stem import WordNetLemmatizer   #Imports the WordNetLemmatizer class from NLTK, which is used to reduce words to their base or root form (lemmatization).\n",
        "\n",
        "# Download NLTK resources (run only once)\n",
        "nltk.download('punkt')       #Downloads the Punkt tokenizer models, which are necessary for tokenization (splitting text into words or sentences).\n",
        "nltk.download('stopwords')   #Downloads the list of stopwords in various languages, including English. This list is used to filter out common, non-informative words.\n",
        "nltk.download('wordnet')     #Downloads the WordNet lexicon, a large lexical database of English, which is used for lemmatization. It helps the lemmatizer determine the base form of words.\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Example text\n",
        "text = \"This is an example sentence, to demonstrate text preprocessing! We'll clean it and tokenize.\"\n",
        "\n",
        "#The text contains punctuation, capital letters, and stopwords, making it a good candidate for cleaning.\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):           #Defines a function preprocess_text that takes a string text as input and performs several preprocessing steps (like lowercasing, tokenization, etc.).\n",
        "    # 1. Lowercase the text\n",
        "    text = text.lower()              #Converts the entire text to lowercase to ensure uniformity, as text processing often ignores case sensitivity. For example, \"Text\" and \"text\" should be treated as the same word.\n",
        "\n",
        "    # 2. Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)      #Uses the re.sub function to remove punctuation and special characters. The regular expression r'[^\\w\\s]' matches any character that is not a word character (\\w) or a whitespace (\\s). This removes things like commas, periods, and exclamation marks.\n",
        "\n",
        "    # 3. Tokenize the text\n",
        "    tokens = word_tokenize(text)        #This splits the text into individual words, resulting in a list of tokens.\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))   #Loads a set of stopwords from the NLTK library for the English language.\n",
        "    tokens = [word for word in tokens if word not in stop_words]   #Filters out the stopwords from the tokenized text.\n",
        "\n",
        "    # 5. Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()  #Creates an instance of the WordNetLemmatizer.\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]    #Applies lemmatization to each token in the list.\n",
        "\n",
        "    return tokens      #Returns the list of preprocessed tokens (cleaned, tokenized, stopword-free, and lemmatized) as the output of the function.\n",
        "\n",
        "# Process the example text\n",
        "cleaned_text = preprocess_text(text)     #Calls the preprocess_text function on the example text. This applies all the preprocessing steps defined earlier, resulting in a cleaned list of tokens.\n",
        "print(\"Cleaned Tokens:\", cleaned_text)   #Prints the cleaned tokens to the console, showing the final result of the preprocessing steps.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9FHzzBW9QMK",
        "outputId": "46648e3e-4869-40df-ba94-925b6acd69ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens: ['example', 'sentence', 'demonstrate', 'text', 'preprocessing', 'well', 'clean', 'tokenize']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:**\n",
        "\n",
        "1. The code demonstrates how to clean and preprocess text using NLTK, covering steps like lowercasing, removing punctuation, tokenization, stopword removal, and lemmatization.\n",
        "\n",
        "2. The preprocessing pipeline is essential for preparing raw text data for tasks such as text classification, sentiment analysis, and other natural language processing tasks."
      ],
      "metadata": {
        "id": "IX9gvvcV9aro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Part-of-speech tagging and Named Entity Recognition**"
      ],
      "metadata": {
        "id": "PRyZXjTxYQyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a Python example using the **spaCy** library for **Part-of-Speech (POS) tagging** and **Named Entity Recognition (NER)**."
      ],
      "metadata": {
        "id": "d4hGm-dTTYhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Background concepts and steps in the code:**\n",
        "\n",
        "**(i)Load spaCy’s model:** en_core_web_sm is a lightweight model that includes tokenization, POS tagging, and NER.\n",
        "\n",
        "**(ii)Part-of-Speech Tagging:** Each word in the text is tagged with its grammatical role, such as noun, verb, adjective, etc. Here’s a simple example:\n",
        "\n",
        "Sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "**PoS Tags:**\n",
        "\n",
        "The - Determiner (DT)\n",
        "\n",
        "quick - Adjective (JJ)\n",
        "\n",
        "brown - Adjective (JJ)\n",
        "\n",
        "fox - Noun (NN)\n",
        "\n",
        "jumps - Verb (VBZ)\n",
        "\n",
        "over - Preposition (IN)\n",
        "\n",
        "the - Determiner (DT)\n",
        "\n",
        "lazy - Adjective (JJ)\n",
        "\n",
        "dog - Noun (NN)\n",
        "\n",
        "\n",
        "Here's a brief overview of other PoS tags for each of these categories:\n",
        "\n",
        "**Pronoun:**\n",
        "\n",
        "PRP (Personal Pronoun): I, you, he, she, it, we, they\n",
        "\n",
        "PRP$ (Possessive Pronoun): my, your, his, her, its, our, their\n",
        "Number:\n",
        "\n",
        "There isn’t a specific PoS tag for \"number\" itself, but numbers are often tagged as:\n",
        "\n",
        "**CD (Cardinal Number):** one, two, three, 42\n",
        "\n",
        "**Adverb:**\n",
        "\n",
        "RB (Adverb): quickly, very, well\n",
        "\n",
        "RBR (Adverb, Comparative): better, faster\n",
        "\n",
        "RBS (Adverb, Superlative): best, fastest\n",
        "\n",
        "**Punctuation:**\n",
        "\n",
        ". (Period) or PUNC (General Punctuation)\n",
        "\n",
        ", (Comma)\n",
        "\n",
        "? (Question Mark)\n",
        "\n",
        "! (Exclamation Mark)\n",
        "\n",
        "**Sign:**\n",
        "\n",
        "SYM (Symbol): $, %, &, @\n",
        "\n",
        "**(iii)Named Entity Recognition:** The named entities in the text are identified and classified.\n",
        "\n",
        "Here is an example:\n",
        "\n",
        "Sentence: \"Barack Obama visited the Eiffel Tower in Paris last summer.\"\n",
        "\n",
        "**NER Tags:**\n",
        "\n",
        "Barack Obama - Person (PER)\n",
        "\n",
        "Eiffel Tower - Location (LOC)\n",
        "\n",
        "Paris - Location (LOC)\n",
        "\n",
        "In this example, NER helps identify and categorize specific names and locations in the text."
      ],
      "metadata": {
        "id": "biI_aSjlRh5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy                         #This imports the spaCy library, which is an advanced NLP library for Python. It provides tools for tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more.\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")    #This loads the small English language model (en_core_web_sm) provided by spaCy. en_core_web_sm is the smallest English model, optimized for speed rather than accuracy.\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion. Steve Jobs founded Apple in 1976.\"\n",
        "\n",
        "#It contains a few named entities (like \"Apple\" and \"Steve Jobs\"), as well as some financial information (like \"$1 billion\") and dates.\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)     #nlp(text) processes the input text and returns a Doc object, which is a container for the processed text.\n",
        "\n",
        "# Part-of-Speech (POS) tagging\n",
        "print(\"Part-of-Speech Tagging:\")  #Part-of-speech (POS) tagging is the task of assigning a grammatical category (like noun, verb, adjective) to each token in the text.\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")  #token.text: This gives the original text of each token (word). token.pos_: This gives the coarse-grained part-of-speech label (e.g., NOUN, VERB, ADJ). token.tag_: This provides a more fine-grained POS tag (e.g., NN for singular noun, VBD for past-tense verb, etc.).\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\") #doc.ents: This contains a list of all the named entities detected in the processed text. ent.text: This gives the text of the named entity (e.g., \"Apple\", \"$1 billion\"). ent.label_: This gives the label for the entity (e.g., ORG for organizations, GPE for geopolitical entities, DATE for dates, MONEY for monetary values). spacy.explain(ent.label_): This provides an explanation of the entity label.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhPEO-DqTp-T",
        "outputId": "29ee8326-da48-41e0-8d83-0b6b21cb9ddc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part-of-Speech Tagging:\n",
            "Apple: PROPN (NNP)\n",
            "is: AUX (VBZ)\n",
            "looking: VERB (VBG)\n",
            "at: ADP (IN)\n",
            "buying: VERB (VBG)\n",
            "U.K.: PROPN (NNP)\n",
            "startup: VERB (VBD)\n",
            "for: ADP (IN)\n",
            "$: SYM ($)\n",
            "1: NUM (CD)\n",
            "billion: NUM (CD)\n",
            ".: PUNCT (.)\n",
            "Steve: PROPN (NNP)\n",
            "Jobs: PROPN (NNP)\n",
            "founded: VERB (VBD)\n",
            "Apple: PROPN (NNP)\n",
            "in: ADP (IN)\n",
            "1976: NUM (CD)\n",
            ".: PUNCT (.)\n",
            "\n",
            "Named Entities:\n",
            "Apple: ORG (Companies, agencies, institutions, etc.)\n",
            "U.K.: GPE (Countries, cities, states)\n",
            "$1 billion: MONEY (Monetary values, including unit)\n",
            "Steve Jobs: PERSON (People, including fictional)\n",
            "Apple: ORG (Companies, agencies, institutions, etc.)\n",
            "1976: DATE (Absolute or relative dates or periods)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary:** This example demonstrates how to use **spaCy** for two key NLP tasks, **POS tagging** and **NER**, which are essential for understanding the structure and meaning of text."
      ],
      "metadata": {
        "id": "KNzA0bp8DY1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Sentiment Analysis**"
      ],
      "metadata": {
        "id": "MmQO4gruee4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of **sentiment analysis** in **Python** using the **TextBlob** library."
      ],
      "metadata": {
        "id": "weElHaqtdjf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Sample text\n",
        "text = \"I absolutely love this product! It's fantastic and works like a charm.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Output sentiment polarity and subjectivity\n",
        "print(f\"Polarity: {sentiment.polarity}\")  # Polarity: -1 (negative) to 1 (positive)\n",
        "print(f\"Subjectivity: {sentiment.subjectivity}\")  # Subjectivity: 0 (objective) to 1 (subjective)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3w_w1pid3Zq",
        "outputId": "cd3bdbc7-e97e-411a-c262-19cd95d5c349"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.5125\n",
            "Subjectivity: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "**(i)Polarity:** Measures the sentiment's positivity or negativity (-1 for negative, 0 for neutral, 1 for positive).\n",
        "\n",
        "**(ii)Subjectivity:** Ranges from 0 (objective) to 1 (subjective), indicating whether the text is based on fact or personal opinion."
      ],
      "metadata": {
        "id": "gUdvlJ6vcAo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the **polarity score** of 0.5125 suggests that the text is **moderately positive** but not overwhelmingly so. The language conveys a generally favorable tone, though it may not be excessively enthusiastic or optimistic.\n",
        "\n",
        "With a **subjectivity score** of 0.75, the text likely contains a **strong personal bias, subjective expressions, or opinions**, rather than being purely based on facts or objective analysis.\n",
        "\n",
        "**Summary:** This code shows that the sample text is moderately positive with a significant degree of subjectivity. **TextBlob** is simple yet effective for basic sentiment analysis tasks."
      ],
      "metadata": {
        "id": "cm8btqhEb5JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Topic Modeling and Document Clustering**"
      ],
      "metadata": {
        "id": "EMBnG29HZbK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a Python example for **Topic Modeling and Document Clustering** using the **Gensim library** and **Latent Dirichlet Allocation (LDA)**.\n",
        "\n",
        "**Topic modeling** is a method used to identify underlying themes or topics within a collection of documents, while **document clustering** focuses on grouping documents that share similar content or themes.\n",
        "\n",
        "**Latent Dirichlet Allocation (LDA)** is a generative statistical model used to discover topics in a collection of documents. It assumes documents are mixtures of topics, and topics are distributions over words.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZN6RoE72fY4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps:**\n",
        "\n",
        "**(i)Preprocessing:** Tokenization, lowercasing, and removal of stopwords.\n",
        "\n",
        "**(ii)Dictionary and Document-Term Matrix:** Created using **Gensim** for converting text data into numerical form.\n",
        "\n",
        "**(iii)LDA Model:** A Latent Dirichlet Allocation model is trained with 2 topics.\n",
        "\n",
        "**(iv)Document Clustering:** Document similarity is demonstrated using cosine similarity between two documents."
      ],
      "metadata": {
        "id": "GZk0hBmaYw_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim                               #Imports the gensim library, which is a popular Python library for topic modeling, document similarity, and vector space modeling. It includes the LDA model, which we’ll use for topic modeling.\n",
        "from gensim import corpora                  #Imports the corpora module from Gensim, which provides utilities for handling a corpus of documents. It includes methods for creating a dictionary (mapping words to unique IDs) and for creating document-term matrices.\n",
        "from gensim.models import LdaModel          #Imports the LdaModel class from Gensim. This class is used for training an LDA model on a corpus to discover topics within a collection of documents.\n",
        "from nltk.corpus import stopwords           #Imports the stopwords corpus from NLTK (Natural Language Toolkit), which contains a list of common words (e.g., \"the\", \"is\", \"and\") that are often removed from text during preprocessing.\n",
        "from nltk.tokenize import word_tokenize     #Imports the word_tokenize function from NLTK, which is used for splitting a sentence into individual words or tokens.\n",
        "import nltk             #Imports the main NLTK library to access other utilities like stopwords and tokenizers.\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download NLTK stopwords (run only once)\n",
        "nltk.download('punkt')       #Downloads the Punkt tokenizer models, which are necessary for word tokenization (splitting sentences into words).\n",
        "nltk.download('stopwords')   #Downloads the stopwords list, which contains a set of common words in English that are typically removed from text before processing.\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Artificial intelligence is transforming the technology industry.\",\n",
        "    \"Machine learning and AI are shaping the future of automation.\",\n",
        "    \"Deep learning algorithms are a subset of machine learning.\",\n",
        "    \"Quantum computing will revolutionize industries like AI.\",\n",
        "    \"Healthcare is benefiting from AI and machine learning advances.\",\n",
        "]\n",
        "\n",
        "#This defines a list of sample documents (sentences) that will be used for topic modeling. These sentences are focused on topics related to artificial intelligence (AI) and machine learning (ML).\n",
        "\n",
        "# Preprocess the documents\n",
        "def preprocess(doc):                                    #Defines a function that preprocesses a document (sentence) to prepare it for modeling by removing stopwords and non-alphabetic words.\n",
        "    stop_words = set(stopwords.words('english'))        #Loads the set of English stopwords from NLTK into the stop_words variable. These are words like \"and\", \"the\", \"is\", etc., that generally do not carry important meaning for topic modeling.\n",
        "    tokens = word_tokenize(doc.lower())                 #Tokenizes the input doc (document) into individual words (tokens) and converts all words to lowercase using .lower() to ensure uniformity (e.g., \"AI\" and \"ai\" will be treated as the same).\n",
        "    return [word for word in tokens if word.isalpha() and word not in stop_words]    #Filters out any tokens that are non-alphabetic (such as punctuation or numbers) and any stopwords. It returns a list of meaningful words (tokens).\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]               #Applies the preprocess() function to each document in the documents list. This results in a list of tokenized, lowercased, stopword-free words for each document.\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "dictionary = corpora.Dictionary(processed_docs)                          #Creates a dictionary using the processed documents. The dictionary maps each unique word (token) to a unique ID. This is an essential step before building a document-term matrix (DTM).\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_docs]    #Converts each preprocessed document into a bag-of-words representation using dictionary.doc2bow(). The doc2bow function converts each document into a list of tuples, where each tuple represents a word ID and its frequency in the document.\n",
        "\n",
        "# Train the LDA model (specifying 2 topics)\n",
        "lda_model = LdaModel(doc_term_matrix, num_topics=2, id2word=dictionary, passes=15)   #Specifies that the model should discover 2 topics from the documents. id2word=dictionary: The dictionary created earlier is passed to the model to help interpret word IDs.. passes=15: Specifies the number of passes (iterations) over the entire corpus to optimize the model. More passes generally result in better topic quality but take more time.\n",
        "\n",
        "# Print the topics with associated words\n",
        "print(\"Topics discovered by LDA:\")\n",
        "topics = lda_model.print_topics(num_words=5)    #Prints the top 5 words for each discovered topic. This allows you to understand what each topic is about based on the most common words in the topic.\n",
        "for topic in topics:\n",
        "    print(topic)        #Iterates over the topics and prints them out. Each topic consists of a list of words that are highly associated with that topic.\n",
        "\n",
        "# Document similarity (clustering example)\n",
        "doc1_bow = dictionary.doc2bow(preprocess(\"AI and machine learning are advancing rapidly\"))   #Preprocesses the new document, converts it into a bag-of-words format using the dictionary, and stores it in doc1_bow.\n",
        "doc2_bow = dictionary.doc2bow(preprocess(\"Healthcare is benefiting from AI advances\"))       #Preprocesses and converts the second document into a bag-of-words representation, storing it in doc2_bow.\n",
        "\n",
        "similarity = gensim.matutils.cossim(doc1_bow, doc2_bow)   #Computes the cosine similarity between the two document vectors (doc1_bow and doc2_bow). Cosine similarity is a measure of similarity between two vectors based on the cosine of the angle between them. A higher cosine value indicates more similarity.\n",
        "print(\"\\nDocument Similarity (cosine):\", similarity)   #Prints the cosine similarity value between the two documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JbGs6QHX5cs",
        "outputId": "fdce557d-ba34-440e-ac51-128a2736eacb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics discovered by LDA:\n",
            "(0, '0.112*\"ai\" + 0.065*\"industries\" + 0.065*\"quantum\" + 0.065*\"computing\" + 0.065*\"revolutionize\"')\n",
            "(1, '0.129*\"learning\" + 0.092*\"machine\" + 0.053*\"subset\" + 0.053*\"deep\" + 0.053*\"algorithms\"')\n",
            "\n",
            "Document Similarity (cosine): 0.2886751345948129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "In this example, the **Gensim library** is used for both topic modeling and document clustering. We begin by preparing a list of sample documents, then create a dictionary and a document-term matrix from them. After that, a Latent Dirichlet Allocation (LDA) model is trained, specifying how many topics we aim to identify. The `print_topics` method is used to display the discovered topics along with their associated words. Additionally, document similarity is demonstrated by converting two documents into bag-of-words vectors and calculating their cosine similarity using `gensim.matutils.cossim`. These examples provide a basic introduction to NLP with Python, setting the stage for more advanced techniques such as text generation, machine translation, and question answering.\n",
        "\n",
        "**Topic ID 0** represents a topic in which words like \"learning\", \"machine\", \"ai\", \"algorithms\", and \"deep\" are highly associated.\n",
        "The numbers before the words (e.g., 0.170, 0.130) represent the weight or importance of each word in this topic.\n",
        "\"learning\" has the highest weight (0.170), meaning it's the most important word for this topic.\n",
        "The topic likely represents discussions about machine learning, deep learning, and related algorithms.\n",
        "\n",
        "**Topic ID 1** is characterized by words like \"ai\", \"revolutionize\", \"industries\", \"like\", and \"quantum\".\n",
        "In this case, \"ai\" appears as the most important word, with a weight of 0.083, and it is closely associated with words that suggest the impact or transformation of industries, likely in the context of AI's potential to revolutionize various sectors.\n",
        "The word \"quantum\" is also prominent, which might indicate a topic that discusses both AI and quantum computing.\n",
        "\n",
        "A **cosine similarity** score of 0.289 (approximately) indicates a moderate to low level of similarity between two documents. This means the documents share some overlap in terms of words or content, but they are more different than similar."
      ],
      "metadata": {
        "id": "ImL9fyLwYEsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remember:** Cosine similarity is a measure of how similar two documents are, based on the angle between their vector representations in a multi-dimensional space. It ranges from -1 to 1, where:\n",
        "\n",
        "(i) 1 indicates that the documents are identical in terms of their direction (completely similar).\n",
        "\n",
        "(ii) 0 means the documents are completely different and share no similarities.\n",
        "\n",
        "(iii) -1 suggests that the documents are opposites in meaning."
      ],
      "metadata": {
        "id": "29QM9Ka38DfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions:**\n",
        "\n",
        "Natural Language Processing (NLP) using Python empowers machines to understand and interpret human language. With Python's vast array of NLP libraries, one can perform various tasks, including cleaning and tokenizing text, analyzing sentiment, modeling topics, and clustering documents. These tools offer immense capabilities for developing intelligent language-based applications."
      ],
      "metadata": {
        "id": "pCPNfPRH7UC8"
      }
    }
  ]
}