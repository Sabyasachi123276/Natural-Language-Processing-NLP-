{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here, we are going to learn about the following topics:\n",
        "\n",
        "(i) SplitÂ andÂ filter text data in preparation for analysis.\n",
        "\n",
        "(ii) Analyze word frequency.\n",
        "\n",
        "(iii) Find concordanceÂ andÂ collocations using different methods.\n",
        "\n",
        "(iv) Perform quick sentiment analysis with NLTKâ€™s built-in VADER.\n",
        "\n",
        "(v) Define features for custom classification.\n",
        "\n",
        "(vi) Use and compare classifiers from scikit-learn for sentiment analysis within NLTK."
      ],
      "metadata": {
        "id": "_ykQSvHzBPUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis helps us determine the ratio of positive to negative engagement about a specific topic. We can analyze bodies of text, such as comments, tweets, and product reviews, to obtain insights from our audience."
      ],
      "metadata": {
        "id": "iCsMtyBtCmCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, use !pipÂ to install NLTK."
      ],
      "metadata": {
        "id": "SKFUhU7xDsPi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crnYh_xQ6k0K",
        "outputId": "141b2f7d-80c9-4bda-d881-61ac88adae38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing the NLTK module, we need to obtain a few additional resources, and we use nltk.download()."
      ],
      "metadata": {
        "id": "NkTX_CcHEM7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "CyajW0Wc7auE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download([\n",
        "...     \"names\",\n",
        "...     \"stopwords\",\n",
        "...     \"state_union\",\n",
        "...     \"twitter_samples\",\n",
        "...     \"movie_reviews\",\n",
        "...     \"averaged_perceptron_tagger\",\n",
        "...     \"vader_lexicon\",\n",
        "...     \"punkt\",\n",
        "... ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sXFMfjm7e2J",
        "outputId": "14d83753-d609-4396-ada1-f0c8ce48171f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we shall use\n",
        "\n",
        "\n",
        "**names:** AÂ list of common English names compiled by Mark Kantrowitz.\n",
        "\n",
        "**stopwords:** A list of really common words, like articles, pronouns, prepositions, and conjunctions.\n",
        "\n",
        "**state_union:**Â A sample of transcribedÂ State of the Union addresses by different US presidents, compiled by Kathleen Ahrens.\n",
        "\n",
        "**twitter_samples:** A list of social media phrases posted to Twitter.\n",
        "\n",
        "**movie_reviews:**Â Two thousand movie reviews categorized by Bo Pang and Lillian Lee.\n",
        "\n",
        "**averaged_perceptron_tagger:**Â A data model that NLTK uses to categorize words into theirÂ parts of speech.\n",
        "\n",
        "**vader_lexicon:**Â A scoredÂ list of words and jargon that NLTK references when performing sentiment analysis, created by C.J. Hutto and Eric Gilbert.\n",
        "\n",
        "**punkt:** A data model created by Jan Strunk that NLTK uses to split full texts into word lists."
      ],
      "metadata": {
        "id": "60LxWdTbE75D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Shakespeare** corpus contains a set of Shakespeare plays, formatted as XML files."
      ],
      "metadata": {
        "id": "x6kTRCo_GFps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "w=nltk.download('shakespeare')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm0im--n7rhR",
        "outputId": "bbe52620-63aa-4c98-d42e-7759287cdbe0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]   Package shakespeare is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compiling Data:** Let us start by loading the State of the Union corpus we downloaded earlier."
      ],
      "metadata": {
        "id": "KwbfGsXhGLFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]"
      ],
      "metadata": {
        "id": "Y3jMmrL676NS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK provides a small corpus of stop words that we can load into a list."
      ],
      "metadata": {
        "id": "BosPQdxeGqug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "bV18_om18BYr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can remove stop words from our original word list."
      ],
      "metadata": {
        "id": "nSFCjH7XGxtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w for w in words if w.lower() not in stopwords]"
      ],
      "metadata": {
        "id": "bherfT2n8Goo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use it, callÂ word_tokenize() with the raw text we want to split."
      ],
      "metadata": {
        "id": "qpWoU5HYHGl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "text = \"\"\"\n",
        "... For some quick analysis, creating a corpus could be overkill.\n",
        "... If all you need is a word list,\n",
        "... there are simpler ways to achieve that goal.\"\"\"\n",
        "pprint(nltk.word_tokenize(text), width=79, compact=True)\n",
        "['For', 'some', 'quick', 'analysis', ',', 'creating', 'a', 'corpus', 'could',\n",
        " 'be', 'overkill', '.', 'If', 'all', 'you', 'need', 'is', 'a', 'word', 'list',\n",
        " ',', 'there', 'are', 'simpler', 'ways', 'to', 'achieve', 'that', 'goal', '.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEJT6yoz8Oyn",
        "outputId": "87d681b5-749f-4262-c18c-d21eee3f2abd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['...', 'For', 'some', 'quick', 'analysis', ',', 'creating', 'a', 'corpus',\n",
            " 'could', 'be', 'overkill', '.', '...', 'If', 'all', 'you', 'need', 'is', 'a',\n",
            " 'word', 'list', ',', '...', 'there', 'are', 'simpler', 'ways', 'to',\n",
            " 'achieve', 'that', 'goal', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['For',\n",
              " 'some',\n",
              " 'quick',\n",
              " 'analysis',\n",
              " ',',\n",
              " 'creating',\n",
              " 'a',\n",
              " 'corpus',\n",
              " 'could',\n",
              " 'be',\n",
              " 'overkill',\n",
              " '.',\n",
              " 'If',\n",
              " 'all',\n",
              " 'you',\n",
              " 'need',\n",
              " 'is',\n",
              " 'a',\n",
              " 'word',\n",
              " 'list',\n",
              " ',',\n",
              " 'there',\n",
              " 'are',\n",
              " 'simpler',\n",
              " 'ways',\n",
              " 'to',\n",
              " 'achieve',\n",
              " 'that',\n",
              " 'goal',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Frequency Distributions:** To build a frequency distribution with NLTK, construct the nltk.FreqDist class with a word list."
      ],
      "metadata": {
        "id": "CrefTmMJHWIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words: list[str] = nltk.word_tokenize(text)\n",
        "fd = nltk.FreqDist(words)"
      ],
      "metadata": {
        "id": "mYRnb-Uk8VyB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After building the object, we can use methods like .most_common()Â andÂ .tabulate()Â to start visualizing information."
      ],
      "metadata": {
        "id": "og5gyDGlHwZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fd.most_common(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox0EKN4Q8a7h",
        "outputId": "07a3f3ec-b503-4c11-d0f8-c5ae1d125bd1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('...', 3), (',', 2), ('a', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd.tabulate(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzlWl_Rr8e7h",
        "outputId": "42bd10dd-5525-4ac1-9a6e-7e290b68705e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...   ,   a \n",
            "  3   2   2 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These methods allow us to quickly determine frequently used words in a sample. With .most_common(), we get a list of tuples containing each word and how many times it appears in our text. We can get the same information in a more readable format with .tabulate()."
      ],
      "metadata": {
        "id": "Vuddz24fINQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to these two methods, we can use frequency distributions to query particular words. These return values indicate the number of times each word occurs exactly as given."
      ],
      "metadata": {
        "id": "XOD0M_xsIlkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fd[\"America\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwiCO2Hi8gv5",
        "outputId": "35b06d25-c406-4aee-b45e-11092d270a5a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd[\"america\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcYSPCGw8qeJ",
        "outputId": "646ddec3-6986-40af-e5a6-dda753f955a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd[\"AMERICA\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V82pCS4l8vqh",
        "outputId": "93d2b562-0006-4399-dc46-6cb0086f7a93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create a new frequency distribution that is based on the initial one but normalizes all words to lowercase."
      ],
      "metadata": {
        "id": "B3EsODv_IwhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_fd = nltk.FreqDist([w.lower() for w in fd])"
      ],
      "metadata": {
        "id": "DEyBl6iZ84YR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting Concordance and Collocations:**"
      ],
      "metadata": {
        "id": "VsLHS67cJRIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before invoking .concordance(), build a new word list from the original corpus text so that all the context, even stop words, will be there."
      ],
      "metadata": {
        "id": "gqes8grEOZHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = nltk.Text(nltk.corpus.state_union.words())\n",
        "text.concordance(\"america\", lines=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPYI1ko989mJ",
        "outputId": "84257ea9-7c21-42ef-9d2b-4ad8b938316a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 5 of 1079 matches:\n",
            " would want us to do . That is what America will do . So much blood has already\n",
            "ay , the entire world is looking to America for enlightened leadership to peace\n",
            "beyond any shadow of a doubt , that America will continue the fight for freedom\n",
            " to make complete victory certain , America will never become a party to any pl\n",
            "nly in law and in justice . Here in America , we have labored long and hard to \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that .concordance() already ignores the case, allowing us to see the context of all case variants of a word in order of appearance. Note also that this function doesnâ€™t show you the location of each word in the text.\n",
        "\n",
        "Moreover, since .concordance() only prints information to the console, itâ€™s not ideal for data manipulation. To obtain a usable list that will also give us information about the location of each occurrence, use .concordance_list().\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QeNr_UTGPH4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concordance_list = text.concordance_list(\"america\", lines=2)\n",
        "for entry in concordance_list:\n",
        "   print(entry.line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8uHFQ_L9A4h",
        "outputId": "5e8edc1e-8eb6-4cee-b81b-694179ba0938"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " would want us to do . That is what America will do . So much blood has already\n",
            "ay , the entire world is looking to America for enlightened leadership to peace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".concordance_list() gives us a list of ConcordanceLineÂ objects, which contain information about where each word occurs as well as a few more properties worth exploring. The list is also sorted in order of appearance.\n",
        "\n",
        "TheÂ nltk.TextÂ class itself has a few other interesting features. One of them isÂ .vocab(), which is worth mentioning because it creates a frequency distribution for a given text.\n",
        "\n",
        "RevisitingÂ nltk.word_tokenize(), check out how quickly we can create a custom nltk. Text instance and an accompanying frequency distribution."
      ],
      "metadata": {
        "id": "Bg3DgXYKPwFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words: list[str] = nltk.word_tokenize(\n",
        "...     \"\"\"Beautiful is better than ugly.\n",
        "...     Explicit is better than implicit.\n",
        "...     Simple is better than complex.\"\"\"\n",
        "... )"
      ],
      "metadata": {
        "id": "gxF4GPxq9I_o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = nltk.Text(words)"
      ],
      "metadata": {
        "id": "3LpOi1Aq9N_o"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd = text.vocab()"
      ],
      "metadata": {
        "id": "x6H3scXR9RuJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd.tabulate(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euDL79Io9YfA",
        "outputId": "aad4d28b-f4a2-48b4-e0d2-9c78dec7e21a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    is better   than \n",
            "     3      3      3 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK provides specific classes for us to find collocations in our text. Following the pattern we have seen so far, these classes are also built from lists of words."
      ],
      "metadata": {
        "id": "wyB1mFETQfzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
        "finder = nltk.collocations.TrigramCollocationFinder.from_words(words)"
      ],
      "metadata": {
        "id": "96CR5v-09aF4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TrigramCollocationFinder instance will search specifically for trigrams."
      ],
      "metadata": {
        "id": "6WQvS8RxQrls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of their most useful tools is the ngram_fd property. This property holds a frequency distribution that is built for each collocation rather than for individual words.\n",
        "\n",
        "Using ngram_fd, we can find the most common collocations in the supplied text."
      ],
      "metadata": {
        "id": "_fB6yo3FQzPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finder.ngram_fd.most_common(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hfgsyOb9l9B",
        "outputId": "1f1db408-e1a1-495e-97ef-db390e3b6a5a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('the', 'United', 'States'), 294), (('the', 'American', 'people'), 185)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finder.ngram_fd.tabulate(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t9SVymY9rex",
        "outputId": "48f8f0c6-0901-4f74-9535-54291c95e22a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ('the', 'United', 'States') ('the', 'American', 'people') \n",
            "                          294                           185 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using NLTKâ€™s Pre-Trained Sentiment Analyzer:** NLTK already has a built-in, pretrained sentiment analyzer called **VADER** (Valence Aware Dictionary and sEntiment Reasoner). To use VADER, first create an instance of nltk.sentiment. SentimentIntensityAnalyzer, then use .polarity_scores() on a raw string.\n",
        "\n"
      ],
      "metadata": {
        "id": "iC6vjgkWRBQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "JPDIu5yM9tiZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSeSwBfw92e5",
        "outputId": "81bf39e0-b008-4ed6-9913-de8cfde9aa37"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shall get back a dictionary of different scores. The negative, neutral, and positive scores are related: They all add up to 1 and cannot be negative. The compound score is calculated differently. It is not just an average, and it can range from -1 to 1.\n",
        "\n",
        "Now we shall put it to the test against real data using two different corpora. First, load the twitter_samples corpus into a list of strings, making a replacement to render URLs inactive to avoid accidental clicks."
      ],
      "metadata": {
        "id": "4-y3yHtRR6cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]"
      ],
      "metadata": {
        "id": "FOarPPcF96jx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use the .polarity_scores() function of our SentimentIntensityAnalyzer instance to classify tweets."
      ],
      "metadata": {
        "id": "uS1AerEWSck6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "\n",
        "def is_positive(tweet: str) -> bool:\n",
        "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
        "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
        "\n",
        "shuffle(tweets)\n",
        "for tweet in tweets[:10]:\n",
        "    print(\">\", is_positive(tweet), tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPhHo-Op99oi",
        "outputId": "1fbd1088-297d-4555-a4a5-c94c914ebc2f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> True @GFuelEnergy i want that but i dont have paypal :(\n",
            "> False Nicola Sturgeon eats English babies for breakfast\n",
            "> True @candinam Gals also bend down in one knees to put a men down and bend down in two knees to pleasure a men up to the heaven :)\n",
            "> False RT @matt_isom: @DavidGWrigley @BBCNews @bbcnickrobinson @Ed_Miliband he slipped on David Cameron's sweat\n",
            "> True RT @HTScotPol: Good grief. Ed Miliband hands SNP massive gift on #bbcqt by saying he'd rather Tories ran country than do a deal with #SNP\n",
            "> False RT @Ed_Miliband: Working families can't afford five more years of the Tories, but in seven days time people can vote Labour to put working â€¦\n",
            "> True RT @OwenJones84: Woohoo! Ed Miliband pointing out that social security bill being fuelled by subsidies for low wages! The facts banished frâ€¦\n",
            "> True @nathan3205 OMG THAT CAME FAST! You graduate at the end of the year?! I know :( catch ups are a definite (what we did best at uni anyways)\n",
            "> False RT @bigbuachaille: Miliband fallout:\n",
            "\n",
            "Westminster election, now officially an English election.\n",
            "> True Go for it, @Nigel_Farage. ðŸ‘ðŸ‘ Awesome and honest. People need to remember why this debate is on tonight in the first place... #bbcqt #UKIP ðŸ‡¬ðŸ‡§\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since VADER needs raw strings for its rating, we cannot use .words() like we did earlier. Instead, make a list of the file IDs that the corpus uses, which we can use later to reference individual reviews."
      ],
      "metadata": {
        "id": "VPecr-3mSo0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
        "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
        "all_review_ids = positive_review_ids + negative_review_ids"
      ],
      "metadata": {
        "id": "rEZiOocq-CVh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, redefine is_positive() to work on an entire review. We shall need to obtain that specific review using its file ID and then split it into sentences before rating."
      ],
      "metadata": {
        "id": "Su_9jTYxS5RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "def is_positive(review_id: str) -> bool:\n",
        "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
        "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
        "    scores = [\n",
        "        sia.polarity_scores(sentence)[\"compound\"]\n",
        "        for sentence in nltk.sent_tokenize(text)\n",
        "    ]\n",
        "    return mean(scores) > 0"
      ],
      "metadata": {
        "id": "KftQiqgH-Ivh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".raw() is another method that exists in most corpora. By specifying a file ID or a list of file IDs, we can obtain specific data from the corpus. Here, we get a single review, then use nltk.sent_tokenize() to obtain a list of sentences from the review. Finally, is_positive() calculates the average compound score for all sentences and associates a positive result with a positive review.\n",
        "\n",
        "We can take the opportunity to rate all the reviews and see how accurate VADER is with this setup."
      ],
      "metadata": {
        "id": "YVs0Y3UxTKyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle(all_review_ids)\n",
        "correct = 0\n",
        "for review_id in all_review_ids:\n",
        "     if is_positive(review_id):\n",
        "         if review_id in positive_review_ids:\n",
        "             correct += 1\n",
        "     else:\n",
        "       if review_id in negative_review_ids:\n",
        "             correct += 1\n",
        "\n",
        "print(F\"{correct / len(all_review_ids):.2%} correct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDvZiD6V-Nsx",
        "outputId": "97afa97e-ab9e-4092-e96f-797505055043"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64.00% correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customizing NLTKâ€™s Sentiment Analysis:**"
      ],
      "metadata": {
        "id": "FT-WpJb7ThCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Selecting Useful Features\n",
        "\n",
        "\n",
        "\n",
        "By using the predefined categories in the movie_reviews corpus, we can create sets of positive and negative words, then determine which ones occur most frequently across each set. Begin by excluding unwanted words and building the initial category groups."
      ],
      "metadata": {
        "id": "z1w9fSnZTtJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
        "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
        "\n",
        "def skip_unwanted(pos_tuple):\n",
        "    word, tag = pos_tuple\n",
        "    if not word.isalpha() or word in unwanted:\n",
        "        return False\n",
        "    if tag.startswith(\"NN\"):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "positive_words = [word for word, tag in filter(\n",
        "    skip_unwanted,\n",
        "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
        ")]\n",
        "negative_words = [word for word, tag in filter(\n",
        "    skip_unwanted,\n",
        "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
        ")]"
      ],
      "metadata": {
        "id": "DkvXTX7S-kKR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to call pos_tag() before filtering our word lists so that NLTK can more accurately tag all words. skip_unwanted(), then uses those tags to exclude nouns, according to NLTKâ€™s default tag set.\n",
        "\n",
        "Now we are ready to create the frequency distributions for our custom feature. Since many words are present in both positive and negative sets, begin by finding the common set so we can remove it from the distribution objects."
      ],
      "metadata": {
        "id": "8w8ddypuT_45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_fd = nltk.FreqDist(positive_words)\n",
        "negative_fd = nltk.FreqDist(negative_words)\n",
        "\n",
        "common_set = set(positive_fd).intersection(negative_fd)\n",
        "\n",
        "for word in common_set:\n",
        "    del positive_fd[word]\n",
        "    del negative_fd[word]\n",
        "\n",
        "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
        "top_100_negative = {word for word, count in negative_fd.most_common(100)}"
      ],
      "metadata": {
        "id": "9S8QWYYg_ICa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how we can set up the positive and negative bigram finders."
      ],
      "metadata": {
        "id": "taA7GvXcUoQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
        "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
        "\n",
        "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
        "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
        "    if w.isalpha() and w not in unwanted\n",
        "])\n",
        "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
        "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
        "    if w.isalpha() and w not in unwanted\n",
        "])"
      ],
      "metadata": {
        "id": "91a4soYs_P1q"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Training and Using a Classifier\n",
        "\n",
        " Since we are looking for positive movie reviews, focus on the features that indicate positivity, including VADER scores."
      ],
      "metadata": {
        "id": "I0crY3x7Uz03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(text):\n",
        "    features = dict()\n",
        "    wordcount = 0\n",
        "    compound_scores = list()\n",
        "    positive_scores = list()\n",
        "\n",
        "    for sentence in nltk.sent_tokenize(text):\n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            if word.lower() in top_100_positive:\n",
        "                wordcount += 1\n",
        "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
        "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
        "\n",
        "    # Adding 1 to the final compound score to always have positive numbers\n",
        "    # since some classifiers you'll use later don't work with negative numbers.\n",
        "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
        "    features[\"mean_positive\"] = mean(positive_scores)\n",
        "    features[\"wordcount\"] = wordcount\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "JR9L1ASl_fgy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "extract_features() should return a dictionary, and it will create three features for each piece of text:\n",
        "\n",
        "1. The average compound score\n",
        "\n",
        "2. The average positive score\n",
        "\n",
        "3. The amount of words in the text that are also part of the top 100 words in all positive reviews\n",
        "\n",
        "In order to train and evaluate a classifier, we shall need to build a list of features for each text we shall analyze."
      ],
      "metadata": {
        "id": "e6KKuljlVNL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
        "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
        "]\n",
        "features.extend([\n",
        "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
        "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
        "])"
      ],
      "metadata": {
        "id": "kJIe2s42_hzi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the classifier involves splitting the feature set so that one portion can be used for training and the other for evaluation, then calling .train()."
      ],
      "metadata": {
        "id": "YBkRm-6-VpQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 1/4 of the set for training\n",
        "train_count = len(features) // 4\n",
        "shuffle(features)\n",
        "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
        "classifier.show_most_informative_features(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBubPh3Z_pDa",
        "outputId": "e56d3d9a-281e-42c8-c550-56d54a2759e0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "               wordcount = 3                 pos : neg    =      4.5 : 1.0\n",
            "               wordcount = 2                 pos : neg    =      3.8 : 1.0\n",
            "               wordcount = 5                 pos : neg    =      3.2 : 1.0\n",
            "               wordcount = 1                 pos : neg    =      2.0 : 1.0\n",
            "               wordcount = 0                 neg : pos    =      1.8 : 1.0\n",
            "               wordcount = 4                 pos : neg    =      1.1 : 1.0\n",
            "           mean_positive = 0.159             pos : neg    =      1.0 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.classify.accuracy(classifier, features[train_count:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXmcEq7G_4cz",
        "outputId": "7ef6fb85-fb57-4ae4-be4b-c01cc085dc50"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6553333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing Additional Classifiers:**"
      ],
      "metadata": {
        "id": "2Wefc2UgWKIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like NLTK, scikit-learn is a third-party Python library, so we shall have to install it with !pip."
      ],
      "metadata": {
        "id": "69yh075JWZGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWJxH2EzAM3M",
        "outputId": "0f51918b-55d7-499e-b560-58667086bc46"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following classifiers are a subset of all classifiers available to us. These will work within NLTK for sentiment analysis."
      ],
      "metadata": {
        "id": "8-f6KLHFWjk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import (\n",
        "    BernoulliNB,\n",
        "    ComplementNB,\n",
        "    MultinomialNB,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
      ],
      "metadata": {
        "id": "3y2lQOC1AQ4L"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To aid in accuracy evaluation, it is helpful to have a mapping of classifier names and their instances."
      ],
      "metadata": {
        "id": "94qSSVrGWvGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {\n",
        "    \"BernoulliNB\": BernoulliNB(),\n",
        "    \"ComplementNB\": ComplementNB(),\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
        "    \"LogisticRegression\": LogisticRegression(),\n",
        "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
        "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
        "}"
      ],
      "metadata": {
        "id": "wiBlAZn7AXbr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using scikit-learn Classifiers With NLTK:** We shall also be able to leverage the same features list we built earlier by means of extract_features(). To refresh our memory, here is how we built the features list."
      ],
      "metadata": {
        "id": "Yj2YUJVjW7Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
        "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
        "]\n",
        "features.extend([\n",
        "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
        "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
        "])"
      ],
      "metadata": {
        "id": "jMXk5g0BAbgj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the first half of the list contains only positive reviews, begin by shuffling it, then iterate over all classifiers to train and evaluate each one."
      ],
      "metadata": {
        "id": "4T3ZVKiAXldE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use 1/4 of the set for training\n",
        "train_count = len(features) // 4\n",
        "shuffle(features)\n",
        "for name, sklearn_classifier in classifiers.items():\n",
        "     classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
        "     classifier.train(features[:train_count])\n",
        "     accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
        "     print(F\"{accuracy:.2%} - {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQh7Q239AkRE",
        "outputId": "ffa4cf52-4827-4529-e025-301de0cbc550"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66.20% - BernoulliNB\n",
            "66.20% - ComplementNB\n",
            "66.20% - MultinomialNB\n",
            "68.93% - KNeighborsClassifier\n",
            "63.33% - DecisionTreeClassifier\n",
            "67.60% - RandomForestClassifier\n",
            "71.47% - LogisticRegression\n",
            "71.73% - MLPClassifier\n",
            "71.20% - AdaBoostClassifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have reached 71.73 percent accuracy before even adding a second feature! While this does not mean that the MLPClassifier will continue to be the best one as we engineer new features, having additional classification algorithms at our disposal is clearly advantageous."
      ],
      "metadata": {
        "id": "uV_xs8TBX0wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "We are now quite familiar with the features of NTLK that allow us to process text into objects that we can filter and manipulate, which allows us to analyze text data to gain information about its properties. We can also use different classifiers to perform sentiment analysis on our data and gain insights into how our audience is responding to content."
      ],
      "metadata": {
        "id": "1f8kUy0wBLhU"
      }
    }
  ]
}